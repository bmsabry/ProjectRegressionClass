knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(glmnet)
library(mlbench)
library(psych)
library(mlbench)
library(GGally)
library(dplyr)
# Data
data("mtcars")
datamtcars <- mtcars
plot1<-ggplot(datamtcars, aes(x=am, y=mpg, color=am)) + geom_boxplot() +geom_point()
# the plot shows a signifcant difference in the data set between both groups and it shows that the data points do cover well the range
# We run a t test to check the statistical significance in teh difference between both groups
significance<-t.test(datamtcars$mpg ~ datamtcars$am)
# the p value shows that there is more than 98% probability they are different, let us do some plots
par(mfrow=c(3,2))
plot2<-ggplot(datamtcars, aes(x=disp, y=mpg, color=am, shape=cyl)) +geom_point()
plot3<-ggplot(datamtcars, aes(x=hp, y=mpg, color=am, shape=gear)) +geom_point()
plot4<-ggplot(datamtcars, aes(x=drat, y=mpg, color=am, shape=vs)) +geom_point()
plot5<-ggplot(datamtcars, aes(x=wt, y=mpg, color=am, shape=cyl)) +geom_point()
plot6<-ggplot(datamtcars, aes(x=qsec, y=mpg, color=am, shape=carb)) +geom_point()
plot2
library(caret)
library(glmnet)
library(mlbench)
library(psych)
library(mlbench)
library(GGally)
library(dplyr)
# Data
data("mtcars")
datamtcars <- mtcars
knitr::opts_chunk$set(echo = TRUE)
# Libraries Needed
library(caret)
library(glmnet)
library(mlbench)
library(psych)
library(mlbench)
library(GGally)
library(dplyr)
# Data
data("mtcars")
datamtcars <- mtcars
str(datamtcars)
# data structure is showing a lot of numerical while they should be listed as factors so we will correct
# that
# variables to be changed to factors
cols <- c("cyl", "vs", "am", "gear","carb")
datamtcars[cols] <- lapply(datamtcars[cols], factor)  ## changed to factor
# checking our work
str(datamtcars)
# searching the data for NA
NAdata <- sum(is.na(datamtcars))
#NA percentage in training set
percentNAdata <- NAdata / (dim(datamtcars)[1] * dim(datamtcars)[2])
percentNAdata
# no NA data in the data set
# Finding near zero and zerp covariates
nsv<-nearZeroVar(datamtcars,saveMetrics = TRUE)
# Which variables are near zero variance. Confirm if physics dictate that they are not important
nsv[nsv[,"nzv"] > 0, ]
# Which variables are zero. Confirm if physics dictate that they are not important
nsv[nsv[,"zeroVar"] > 0, ]
# There are no near zero or zero variables
# now we remove factors to study collinearity of variables
names.use <- names(datamtcars)[!(names(datamtcars) %in% cols)]
datamtcars.subset <- datamtcars[, names.use]
# # running correlation
descrCor <-  cor(mtcars)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .9)
# No high correlation beyond 0.9 so we willnot remove any variable for now
# plotting data pairs to have a first look at the variation
ggpairs(datamtcars, aes(colour = am, alpha = 0.4))
plot1<-ggplot(datamtcars, aes(x=am, y=mpg, color=am)) + geom_boxplot() +geom_point()
# the plot shows a signifcant difference in the data set between both groups and it shows that the data points do cover well the range
# We run a t test to check the statistical significance in teh difference between both groups
significance<-t.test(datamtcars$mpg ~ datamtcars$am)
# the p value shows that there is more than 98% probability they are different, let us do some plots
par(mfrow=c(3,2))
plot2<-ggplot(datamtcars, aes(x=disp, y=mpg, color=am, shape=cyl)) +geom_point()
plot3<-ggplot(datamtcars, aes(x=hp, y=mpg, color=am, shape=gear)) +geom_point()
plot4<-ggplot(datamtcars, aes(x=drat, y=mpg, color=am, shape=vs)) +geom_point()
plot5<-ggplot(datamtcars, aes(x=wt, y=mpg, color=am, shape=cyl)) +geom_point()
plot6<-ggplot(datamtcars, aes(x=qsec, y=mpg, color=am, shape=carb)) +geom_point()
plot2
plot3
plot4
plot5
plot6
str(datamtcars)
# the plots show several variables correlate with the mpg, however, the displacment is sufficent to be express the cylinders number so i will remove it from the analysis
datamtcars.subset<-select(datamtcars,-cyl)
plot1<-ggplot(datamtcars, aes(x=am, y=mpg, color=am)) + geom_boxplot() +geom_point()
# the plot shows a signifcant difference in the data set between both groups and it shows that the data points do cover well the range
# We run a t test to check the statistical significance in teh difference between both groups
significance<-t.test(datamtcars$mpg ~ datamtcars$am)
# the p value shows that there is more than 98% probability they are different, let us do some plots
par(mfrow=c(3,2))
plot2<-ggplot(datamtcars, aes(x=disp, y=mpg, color=am, shape=cyl)) +geom_point()
plot3<-ggplot(datamtcars, aes(x=hp, y=mpg, color=am, shape=gear)) +geom_point()
plot4<-ggplot(datamtcars, aes(x=drat, y=mpg, color=am, shape=vs)) +geom_point()
plot5<-ggplot(datamtcars, aes(x=wt, y=mpg, color=am, shape=cyl)) +geom_point()
plot6<-ggplot(datamtcars, aes(x=qsec, y=mpg, color=am, shape=carb)) +geom_point()
plot2
plot3
plot4
plot5
plot6
str(datamtcars)
# the plots show several variables correlate with the mpg, however, the displacment is sufficent to be express the cylinders number so i will remove it from the analysis
datamtcars.subset<-select(datamtcars,-cyl)
# Custom Control Parameters
custom <- trainControl(method = "repeatedcv",
number = 3,
repeats = 2)
lm <- train(mpg~., datamtcars.subset, method='lm',trControl=custom)
# Results and model examination
plot(lm$finalModel)
lm$results
plot(varImp(lm, scale=T))
summary(lm$finalModel)
ridge <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=0, lambda=seq(3,4,length=5)),
trControl=custom)
summary(ridge$finalModel)
# Plot Results and examine model
plot(ridge) #Examine how lamda impacts the model and may be tweek the breaks up in the model based on this
ridge #for model summary numerically
plot(ridge$finalModel, xvar = "lambda", label = T)
plot(ridge$finalModel, xvar = 'dev', label=T) # how much of the data variance is explained
plot(varImp(ridge, scale=T)) # variable relative importance
# Lasso Regression
set.seed(1234)
lasso <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=1, lambda=seq(0.4,1,length=5)),
trControl=custom)
# Plot Results and examine
plot(lasso)
plot(lasso$finalModel, xvar = 'lambda', label=T)
plot(lasso$finalModel, xvar = 'dev', label=T)
# notice the number of variables that are needed to explain
# the data variability.
plot(varImp(lasso, scale=T)) # checking the variables relative importance
# Elastic Net Regression
# IT is essentially a mix of ridge and lasso
set.seed(1234)
en <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=seq(0.1,1,length=10), lambda=seq(0.0001,1,length=5)),
trControl=custom)
# Plot Results
plot(en)
plot(en$finalModel, xvar = 'lambda', label=T)
plot(en$finalModel, xvar = 'dev', label=T)
plot(varImp(best))
# Custom Control Parameters
custom <- trainControl(method = "repeatedcv",
number = 3,
repeats = 2)
lm <- train(mpg~., datamtcars.subset, method='lm',trControl=custom)
# Results and model examination
plot(lm$finalModel)
lm$results
plot(varImp(lm, scale=T))
summary(lm$finalModel)
ridge <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=0, lambda=seq(3,4,length=5)),
trControl=custom)
summary(ridge$finalModel)
# Plot Results and examine model
plot(ridge) #Examine how lamda impacts the model and may be tweek the breaks up in the model based on this
ridge #for model summary numerically
plot(ridge$finalModel, xvar = "lambda", label = T)
plot(ridge$finalModel, xvar = 'dev', label=T) # how much of the data variance is explained
plot(varImp(ridge, scale=T)) # variable relative importance
# Lasso Regression
set.seed(1234)
lasso <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=1, lambda=seq(0.4,1,length=5)),
trControl=custom)
# Plot Results and examine
plot(lasso)
plot(lasso$finalModel, xvar = 'lambda', label=T)
plot(lasso$finalModel, xvar = 'dev', label=T)
# notice the number of variables that are needed to explain
# the data variability.
plot(varImp(lasso, scale=T)) # checking the variables relative importance
# Elastic Net Regression
# IT is essentially a mix of ridge and lasso
set.seed(1234)
en <- train(mpg~., datamtcars.subset,
method='glmnet',
tuneGrid=expand.grid(alpha=seq(0.1,1,length=10), lambda=seq(0.0001,1,length=5)),
trControl=custom)
# Plot Results
plot(en)
plot(en$finalModel, xvar = 'lambda', label=T)
plot(en$finalModel, xvar = 'dev', label=T)
# Compare Models.
model_list <- list(LineraModel=lm,Ridge=ridge,Lassso=lasso,
ElasticNet=en) #create list of models used
res <- resamples(model_list) #using the resamples in carrot package to compare model
summary(res) #numerical comparison between models. Check the mean value of the mean rsquared
bwplot(res) #plotting the models for comparison
xyplot(res,metric='RMSE')
# Best Model
en$bestTune # best values of alpha and lambda
best <- en$finalModel #Saving best model
coef(best, s = en$bestTune$lambda) #coefficents of the best model
# lasso is the best mode. let us compare all coefficients for the parameter in the lm and best model
coefficients(summary(lm))[[8]] # lm
coef(best, s = en$bestTune$lambda)[8] # best lasso
# both models suggest that the increase in mpg if you go from manual to automatic is 1.4-2 mpg
# from an engineering perspective and being a combustion engineer, i beleive that the most important variables are displacement, wt, qsec, hp, am so i will just build a model using that and see how easy it is to interpret the model and feel it.
datamtcars.simple<-select(mtcars,-cyl,-drat,-gear,-carb,-vs)
datamtcars.simple$am<-as.factor(datamtcars.simple$am)
lmSimple <- train(mpg~., datamtcars.simple, method='lm',trControl=custom)
# Results and model examination
plot(lmSimple$finalModel)
lmSimple$results
plot(varImp(lmSimple, scale=T))
summary(lmSimple$finalModel)
# The model has a adjusted Rsquared higher than all other model we build in this project. IT actually suggests that the most important variables to inculde are wt, qsec and am so we will repeat the model with that
lmSimple <- train(mpg~ wt+qsec+am, datamtcars.simple, method='lm',trControl=custom)
# Results and model examination
plot(lmSimple$finalModel)
lmSimple$results
plot(varImp(lmSimple, scale=T))
summary(lmSimple$finalModel)
# The simple models suggests an increase of 2.9 mpg as we switch to automatic transmission
# the diagnostic plots are all reasonable. However, the increase in standardized residuals with the fitted values shows that the model is strongly dependent on data points given.
# Libraries Needed
library(caret)
library(glmnet)
library(mlbench)
library(psych)
library(mlbench)
library(GGally)
library(dplyr)
# Data
data("mtcars")
datamtcars <- mtcars
```{r, cache=TRUE, echo=TRUE, warning=FALSE, eval=TRUE}
install_tinytex()
install.packages("tinytex")
library(tinytex)
install.packages("latexpdf")
library(latexpdf)
install.packages("MikTeX")
Sys.getenv("R_ENVIRON")
Sys.getenv("R_HOME")
tinytex::install_tinytex()
